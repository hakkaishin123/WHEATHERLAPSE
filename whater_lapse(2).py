# -*- coding: utf-8 -*-
"""Whater_lapse.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13IV-66a78yYeyG4eM8y8bWaAvtVYXMaK
"""

# Instalar las librerías del sistema que cartopy necesita
!apt-get install -qq libproj-dev libgdal-dev

!pip install linearmodels

# Instalar las librerías de Python de manera más secuencial para evitar conflictos de dependencia
# Instalar numpy primero
!pip install numpy
# Instalar shapely por separado si hay problemas (aunque con numpy instalado primero debería ser menos problemático)
!pip install --no-binary shapely shapely
# Instalar las demás librerías
!pip install cartopy xarray netcdf4 jupyter_bokeh gdown modin panel hvplot bokeh statsmodels regionmask

import xarray as xr
import modin.pandas as pd
import gdown
import numpy as np
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
from shapely.geometry import Point
import cartopy.feature as cfeature
import geopandas as gpd
import regionmask
import os
from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter
from statsmodels.tsa.seasonal import seasonal_decompose # Importar de nuevo después de corregir numpy/scipy/statsmodels



# --- 1. Configuración y Cajas Delimitadoras ---
output= "colombia_climate.nc" # Renombrado el archivo de salida para mayor claridad
gdown.download(f"https://drive.google.com/uc?id=1rYPyFO0I7Lh7r3KtA_W095K08NIDJcJk", output, quiet=False)
netcdf_filename= output # Usar directamente el nombre del archivo descargado
output_map_filename = 'colombia_all_regions_map.png'

# Definir las cajas delimitadoras de las regiones (esto se sobrescribirá si la celda qxjUNyCUtsBk se ejecuta después)
# Mantengo estas definiciones aquí por si esta celda se ejecuta de forma aislada,
# pero la fuente principal de las cajas delimitadoras es la celda qxjUNyCUtsBk.
regiones_colombia = {
    "Caribe": {'min_lat': 8.0, 'max_lat': 13.0, 'min_lon': -77.5, 'max_lon': -71.0},
    "Pacífica": {'min_lat': 0.5, 'max_lat': 8.5, 'min_lon': -79.5, 'max_lon': -75.5},
    "Andina": {'min_lat': -4.0, 'max_lat': 11.0, 'min_lon': -78.0, 'max_lon': -70.0},
    "Orinoquía": {'min_lat': 2.0, 'max_lat': 8.0, 'min_lon': -73.0, 'max_lon': -66.0},
    "Amazonía": {'min_lat': -5.0, 'max_lat': 3.0, 'min_lon': -76.0, 'max_lon': -68.0},
    "Insular": {'min_lat': 12.2, 'max_lat': 13.8, 'min_lon': -81.5, 'max_lon': -80.5}
}

# --- 1. Load the dataset and the geospatial regions ---

# Path to your GeoPackage file
gpkg_filename = '/content/regiones.gpkg'

# Open the NetCDF dataset using chunks for efficiency
ds = xr.open_dataset(netcdf_filename, chunks='auto')

# Open the GeoPackage file with the real region boundaries
# Assuming the file has a column named 'REGION' or similar to identify each region
regions_gdf = gpd.read_file(gpkg_filename)

print("Dataset successfully loaded:")
print("\nGeoPackage for regions successfully loaded:")
regions_gdf.info()

# IMPORTANT: Ensure your GeoDataFrame has a column with unique names for each region.
region_column_name = 'REGION' # <- CHANGE THIS to the actual column name in your gpkg

# Create a 'Regions' object from your GeoDataFrame.
# This tells regionmask which column contains the names and abbreviations.
# The numbers (integer IDs) are assigned automatically based on the row order.
colombian_regions = regionmask.from_geopandas(
    regions_gdf,
    names=region_column_name,
    abbrevs=region_column_name,
)


# Now, create the mask using this new object.

mask = colombian_regions.mask(ds.longitude, ds.latitude)

ds_masked = ds.assign_coords(region=mask).where(mask.notnull(), drop=True)

output_dir = "parquets"
os.makedirs(output_dir, exist_ok=True)

region_names_map = dict(zip(colombian_regions.numbers, colombian_regions.names))

# The core of the method: group by the 'region' coordinate
# This lazily creates a separate sub-dataset for each region.
grouped_by_region = ds_masked.groupby('region')

print("\nStarting to process and save each region...")

dataframes_regiones = {}

for region_number, region_ds in grouped_by_region:
    # Get the region's text name from our map
    region_name = region_names_map[region_number]
    parquet_path = os.path.join(output_dir, f"{region_name}.parquet")

    # Optional: Skip if already processed
    if os.path.exists(parquet_path):
        print(f"-> Skipping '{region_name}', parquet already exists.")
        continue

    print(f"-> Processing '{region_name}' (ID={region_number})...")

    # `region_ds` is a clean xarray.Dataset for this region only.
    # It contains all time steps for only the grid points inside the polygon.

    # Check if the subset is empty (shouldn't happen with drop=True, but good practice)
    if region_ds.latitude.size == 0:
        print(f" -> No data points found for '{region_name}'.")
        continue

    region_ds_unified = region_ds.unify_chunks()
    # Convert this clean, pre-filtered dataset directly to a Dask DataFrame
    ddf = region_ds_unified.to_dask_dataframe()

    # Save to parquet
    print(f" -> Writing parquet for '{region_name}'...")
    ddf.to_parquet(parquet_path, engine="pyarrow", overwrite=True)


    df = pd.read_parquet(parquet_path)
    dataframes_regiones[region_name] = df
    print(f" -> Successfully wrote '{region_name}.parquet' with shape {df.shape}")

print("\nAll regions processed successfully.")

import shapely
from shapely import contains_xy

# --- Config ---
parquet_dir = "/content/parquets"
region_col = "REGION"
region_names = regions_gdf[region_col].unique().tolist()

# --- Figure setup ---
n = len(region_names)
ncols = 3
nrows = (n + ncols - 1) // ncols
fig, axes = plt.subplots(
    nrows=nrows, ncols=ncols,
    figsize=(5 * ncols, 5 * nrows),
    subplot_kw={"projection": ccrs.PlateCarree()}
)

axes = axes.flatten()

for i, region_name in enumerate(region_names):
    ax = axes[i]
    print(f"\n→ Checking region '{region_name}'...")
    parquet_path = f"{parquet_dir}/{region_name}.parquet"
    df = pd.read_parquet(parquet_path)

    # --- 1. Get region geometry ---
    region_poly = regions_gdf.loc[
        regions_gdf[region_col] == region_name, "geometry"
    ].values[0]

    if region_poly.geom_type == "MultiPolygon":
        region_poly = shapely.union_all(region_poly.geoms)

    # --- 2. Drop duplicates to reduce overplotting ---
    before = len(df)
    df = df.drop_duplicates(subset=["latitude", "longitude"])
    reduction = (before - len(df)) / before
    print(f"   Dropped {before - len(df):,} duplicates ({reduction:.1%} reduction)")

    # --- 3. Inside test ---
    inside_mask = contains_xy(region_poly, df["longitude"].to_numpy(), df["latitude"].to_numpy())
    df["inside"] = inside_mask
    inside_ratio = df["inside"].mean()

    # --- 4. Plot ---
    ax.set_extent([-83, -66, -5, 16], crs=ccrs.PlateCarree())
    ax.add_feature(cfeature.LAND, facecolor="#e0dfd5", edgecolor="black")
    ax.add_feature(cfeature.COASTLINE)
    ax.add_feature(cfeature.BORDERS, linestyle=":")
    ax.add_feature(cfeature.OCEAN, facecolor="#add8e6", zorder=0)

    ax.scatter(
        df.loc[df["inside"], "longitude"],
        df.loc[df["inside"], "latitude"],
        color="green", s=4, alpha=0.6,
        transform=ccrs.PlateCarree(), label="Inside"
    )
    ax.scatter(
        df.loc[~df["inside"], "longitude"],
        df.loc[~df["inside"], "latitude"],
        color="red", s=4, alpha=0.4,
        transform=ccrs.PlateCarree(), label="Outside"
    )

    # Region boundary
    regions_gdf.loc[regions_gdf[region_col] == region_name].plot(
        ax=ax, edgecolor="blue", facecolor="none",
        linewidth=1.5, transform=ccrs.PlateCarree()
    )

    ax.set_title(f"{region_name}\nInside: {inside_ratio:.1%}", fontsize=12)
    ax.legend(loc="lower left", fontsize=8)

# Hide any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.suptitle("Unique Grid Points per Region", fontsize=16)
plt.tight_layout()
plt.show()

# --- 1. Definir las regiones naturales de Colombia ---
# Cada región está representada con una "caja delimitadora" aproximada (latitudes y longitudes mínimas y máximas)
regiones_colombia = {
    "Caribe": {'min_lat': 8.0, 'max_lat': 12.5, 'min_lon': -77.0, 'max_lon': -71.5},
    "Pacífica": {'min_lat': 1.5, 'max_lat': 8.0, 'min_lon': -79.0, 'max_lon': -76.0},
    "Andina": {'min_lat': 1.0, 'max_lat': 8.0, 'min_lon': -77.0, 'max_lon': -72.0},  # max_lat modificado a 8.0
    "Orinoquía": {'min_lat': 3.0, 'max_lat': 7.0, 'min_lon': -72.0, 'max_lon': -67.0},
    "Amazonía": {'min_lat': -4.5, 'max_lat': 2.0, 'min_lon': -75.0, 'max_lon': -67.0},
    "Insular": {'min_lat': 12.0, 'max_lat': 14.0, 'min_lon': -82.0, 'max_lon': -81.0}
}

# --- 2. Abrir el conjunto de datos en formato NetCDF ---
# 'chunks=auto' permite manejar datos grandes dividiéndolos en bloques de manera automática
ds = xr.open_dataset(netcdf_filename, chunks='auto')

# --- 3. Generar el mapa mostrando todas las regiones ---
print("Generando mapa con las cajas delimitadoras de todas las regiones...")
figura = plt.figure(figsize=(15, 12))
eje = figura.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())
eje.set_extent([-83, -66, -5, 16], crs=ccrs.PlateCarree())

# Agregar elementos geográficos
# Tierras
eje.add_feature(cfeature.LAND, zorder=0, edgecolor='black', facecolor='#d2b48c')
# Océanos
eje.add_feature(cfeature.OCEAN, zorder=0, facecolor='#add8e6')
# Costa
eje.add_feature(cfeature.COASTLINE, zorder=1)
# Fronteras
eje.add_feature(cfeature.BORDERS, zorder=1, linestyle=':')

# Dibujar la grilla completa de puntos (latitud/longitud) para contexto
grilla_lon, grilla_lat = np.meshgrid(ds['longitude'].values, ds['latitude'].values)
eje.scatter(grilla_lon, grilla_lat, color='gray', marker='.', s=5, alpha=0.2, label='Grilla ERA5 completa')

# Recorrer cada región y dibujar su caja delimitadora
colores = ['red', 'blue', 'green', 'purple', 'orange', 'cyan']
for i, (nombre_region, caja) in enumerate(regiones_colombia.items()):
    caja_lons = [caja['min_lon'], caja['max_lon'], caja['max_lon'], caja['min_lon'], caja['min_lon']]
    caja_lats = [caja['min_lat'], caja['min_lat'], caja['max_lat'], caja['max_lat'], caja['min_lat']]
    eje.plot(caja_lons, caja_lats, color=colores[i], linewidth=3, linestyle='--',
             label=nombre_region)

# Agregar líneas de grilla y leyenda
grilla = eje.gridlines(draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')
grilla.top_labels = False
grilla.right_labels = False
grilla.xformatter = LongitudeFormatter()
grilla.yformatter = LatitudeFormatter()
eje.set_title("Cajas delimitadoras de las 6 regiones naturales de Colombia", fontsize=16)
eje.legend(loc='upper left', fontsize=10)

# Guardar y mostrar el mapa
plt.savefig(output_map_filename, dpi=300, bbox_inches='tight')
plt.show()

# --- 4. Crear un diccionario de DataFrames para cada región ---
dataframes_regiones = {}
print("\nCortando el dataset y creando un DataFrame para cada región...")
for nombre_region, caja in regiones_colombia.items():
    lat_slice = slice(caja['min_lat'], caja['max_lat'])
    lon_slice = slice(caja['min_lon'], caja['max_lon'])
    region_ds = ds.sel(latitude=lat_slice, longitude=lon_slice)

    # Convertir la región en DataFrame si tiene datos válidos
    if region_ds.latitude.size > 0 and region_ds.longitude.size > 0:
        df = region_ds.to_dataframe().reset_index()
        dataframes_regiones[nombre_region] = df
        print(f"-> DataFrame creado para '{nombre_region}' con dimensiones: {df.shape}")
    else:
        dataframes_regiones[nombre_region] = pd.DataFrame()
        print(f"-> No se encontraron datos para '{nombre_region}'. Se creó un DataFrame vacío.")

print("\nProceso completado.")

def obtener_serie_tiempo_punto(dataframe, lat_objetivo, lon_objetivo):
    """
    Encuentra el punto de grilla más cercano dentro de un DataFrame
    a unas coordenadas latitud/longitud objetivo y retorna su serie de tiempo completa.
    """
    # Calcular la distancia al cuadrado para hallar el punto más cercano
    distancias = (dataframe['latitude'] - lat_objetivo)**2 + (dataframe['longitude'] - lon_objetivo)**2
    indice_cercano = distancias.idxmin()

    # Obtener las coordenadas reales del punto de grilla más cercano
    lat_cercana = dataframe.loc[indice_cercano, 'latitude']
    lon_cercana = dataframe.loc[indice_cercano, 'longitude']

    print(f"Objetivo: {lat_objetivo} N, {lon_objetivo} W. Punto de grilla más cercano encontrado en: {lat_cercana:.2f} N, {lon_cercana:.2f} W")

    # Filtrar el DataFrame para todos los registros de ese único punto
    serie_tiempo_df = dataframe[
        (dataframe['latitude'] == lat_cercana) &
        (dataframe['longitude'] == lon_cercana)
    ].copy()  # .copy() asegura que sea un nuevo DataFrame independiente

    return serie_tiempo_df


# --- Usar la función para obtener datos de Cartagena ---
# Utilizamos el DataFrame de la región "Caribe" del diccionario 'dataframes_regiones'
cartagena_df = obtener_serie_tiempo_punto(
    dataframe=dataframes_regiones['Caribe'],
    lat_objetivo=10.4,
    lon_objetivo=-75.5
)

# Mostrar el resultado
print("\n--- Serie de tiempo completa para el punto de grilla más cercano a Cartagena ---")
print(cartagena_df)

# Convertir la columna de fechas a tipo datetime
cartagena_df['valid_time'] = pd.to_datetime(cartagena_df['valid_time'])

# Encontrar las fechas mínima y máxima del DataFrame
fecha_inicio = cartagena_df['valid_time'].min()
fecha_fin = cartagena_df['valid_time'].max()

print(f"El rango real de fechas en el dataset es: desde {fecha_inicio} hasta {fecha_fin}")

# Filtrar todos los registros del 2 de septiembre a lo largo de los años disponibles
sept_2_todos_los_anios = cartagena_df[
    (cartagena_df['valid_time'].dt.month == 9) &
    (cartagena_df['valid_time'].dt.day == 2)
]

print("--- Datos para el 2 de septiembre en todos los años disponibles ---")
print(sept_2_todos_los_anios)

# --- Generar un mapa mostrando la ubicación del punto de Cartagena ---
print("Generando mapa mostrando la ubicación del punto de Cartagena...")
figura_punto = plt.figure(figsize=(10, 8))
eje_punto = figura_punto.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())

# Establecer extensión alrededor de Cartagena
# Puede que quieras ajustar estos valores para una mejor vista
eje_punto.set_extent([-76.5, -74.5, 9.5, 11.5], crs=ccrs.PlateCarree())


# Agregar elementos geográficos
eje_punto.add_feature(cfeature.LAND, zorder=0, edgecolor='black', facecolor='#d2b48c')
eje_punto.add_feature(cfeature.OCEAN, zorder=0, facecolor='#add8e6')
eje_punto.add_feature(cfeature.COASTLINE, zorder=1)
eje_punto.add_feature(cfeature.BORDERS, zorder=1, linestyle=':')

# Trazar el punto específico para Cartagena
# Obtener las coordenadas de la primera fila del cartagena_df (son todas iguales para este punto)
lat_cartagena = cartagena_df.iloc[0]['latitude']
lon_cartagena = cartagena_df.iloc[0]['longitude']

eje_punto.scatter(lon_cartagena, lat_cartagena, color='red', marker='X', s=100, label='Punto Cartagena')

# Agregar líneas de grilla y leyenda
grilla_punto = eje_punto.gridlines(draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')
grilla_punto.top_labels = False
grilla_punto.right_labels = False
grilla_punto.xformatter = LongitudeFormatter()
grilla_punto.yformatter = LatitudeFormatter()
eje_punto.set_title("Ubicación del punto de grilla más cercano a Cartagena", fontsize=14)
eje_punto.legend(loc='upper right', fontsize=10)

# Mostrar el mapa
plt.show()

# Asumimos que 'cartagena_df' es el DataFrame con la serie de tiempo
# y que la columna 'valid_time' ya está en formato datetime.

# --- 1. Seleccionar los datos de septiembre para dos años distintos ---

# Datos de septiembre 2005
sept_2005_df = cartagena_df[cartagena_df['valid_time'].dt.year == 2005]
sept_2005_df = sept_2005_df[sept_2005_df['valid_time'].dt.month == 9]

# Datos de septiembre 2020
sept_2020_df = cartagena_df[cartagena_df['valid_time'].dt.year == 2020]
sept_2020_df = sept_2020_df[cartagena_df['valid_time'].dt.month == 9]


# --- 2. Graficar ambos años en la misma figura para comparación ---
if not sept_2005_df.empty and not sept_2020_df.empty:
    plt.figure(figsize=(15, 7))

    # Temperatura media en 2005
    plt.plot(sept_2005_df['valid_time'].dt.hour, sept_2005_df['t2m'],
             label='Septiembre 2005 Promedio', color='blue', marker='o')

    # Temperatura media en 2020
    plt.plot(sept_2020_df['valid_time'].dt.hour, sept_2020_df['t2m'],
             label='Septiembre 2020 Promedio', color='red', marker='x')

    plt.title("Comparación del ciclo diario de temperatura promedio en septiembre (2005 vs. 2020)", fontsize=16)
    plt.xlabel("Hora del día (UTC)", fontsize=12)
    plt.ylabel("Temperatura promedio (K)", fontsize=12)  # Unidades de 't2m'
    plt.grid(True, linestyle='--')
    plt.xticks(range(0, 24))  # Mostrar todas las horas como marcas
    plt.legend()
    plt.show()
else:
    print("No se encontraron datos para uno o ambos de los años seleccionados.")

# --- Repetir pero para precipitación total ('tp') ---
if not sept_2005_df.empty and not sept_2020_df.empty:
    plt.figure(figsize=(15, 7))

    # Precipitación total en 2005
    plt.plot(sept_2005_df['valid_time'].dt.hour, sept_2005_df['tp'],
             label='Septiembre 2005 Precipitación Total', color='blue', marker='o')

    # Precipitación total en 2020
    plt.plot(sept_2020_df['valid_time'].dt.hour, sept_2020_df['tp'],
             label='Septiembre 2020 Precipitación Total', color='red', marker='x')

    plt.title("Comparación del ciclo diario de precipitación total en septiembre (2005 vs. 2020)", fontsize=16)
    plt.xlabel("Hora del día (UTC)", fontsize=12)
    plt.ylabel("Precipitación total (m)", fontsize=12)  # Unidades de 'tp'
    plt.grid(True, linestyle='--')
    plt.xticks(range(0, 24))
    plt.legend()
    plt.show()
else:
    print("No se encontraron datos para uno o ambos de los años seleccionados.")

"""# Necesitamos ajustar esto pero para ayer"""

# --- Recargar cartagena_df desde cero para limpieza de datos ---
cartagena_df = obtener_serie_tiempo_punto(
    dataframe=dataframes_regiones['Caribe'],
    lat_objetivo=10.4,
    lon_objetivo=-75.5
)

# Asegurar que el tiempo es tipo datetime y usarlo como índice
cartagena_df['valid_time'] = pd.to_datetime(cartagena_df['valid_time'])
cartagena_df = cartagena_df.set_index('valid_time')

print("Valores faltantes antes del tratamiento:")
print(cartagena_df.isnull().sum())

# Identificar columnas con más del 50% de valores faltantes
nan_counts = cartagena_df.isnull().sum()
cols_muchos_nulos = nan_counts[nan_counts > (len(cartagena_df) * 0.5)].index.tolist()

# Rellenar con 0 las columnas con muchos nulos
print(f"\nRellenando con 0 las columnas con muchos NaN: {cols_muchos_nulos}...")
cartagena_df[cols_muchos_nulos] = cartagena_df[cols_muchos_nulos].fillna(0)

# Identificar columnas con menos del 50% de NaN y que sean numéricas
cols_pocos_nulos = nan_counts[nan_counts <= (len(cartagena_df) * 0.5)].index.tolist()
cols_interpolar = [
    col for col in cols_pocos_nulos
    if cartagena_df[col].dtype in ['float64', 'int64'] and col not in ['latitude', 'longitude', 'number', 'expver']
]

# Interpolar los valores faltantes en esas columnas
print(f"Interpolando columnas con pocos NaN: {cols_interpolar}...")
for col in cols_interpolar:
    cartagena_df[col] = cartagena_df[col].interpolate(method='time')

# Manejar valores faltantes en los extremos con ffill y bfill
cartagena_df = cartagena_df.ffill().bfill()

# Convertir columnas de temperatura de Kelvin a Celsius
temp_cols = ['d2m', 't2m', 'sst', 'skt', 'stl1']
for col in temp_cols:
    if col in cartagena_df.columns:
        cartagena_df[col] = cartagena_df[col] - 273.15
        print(f"Convertido '{col}' de Kelvin a Celsius.")


print("\nValores faltantes después del tratamiento:")
print(cartagena_df.isnull().sum())

print("\nVista previa del DataFrame preparado:")
display(cartagena_df.head())

cartagena_df.describe()

# --- Widgets interactivos con Panel ---
from panel.widgets import Select, DatetimeRangePicker

# Definir variables disponibles para el menú desplegable
# Excluir columnas no numéricas, de tiempo e identificadores
opciones_variables = [col for col in cartagena_df.columns if col not in ['latitude', 'longitude', 'number', 'expver']]

# Widget para seleccionar variable
selector_variable = Select(name='Seleccionar variable', options=opciones_variables)

# Definir rango de fechas basado en el índice del DataFrame
fecha_minima = cartagena_df.index.min().to_pydatetime()
fecha_maxima = cartagena_df.index.max().to_pydatetime()

# Widget para seleccionar rango de fechas
selector_rango_fechas = DatetimeRangePicker(
    name='Seleccionar rango de fechas',
    start=fecha_minima,
    end=fecha_maxima,
    value=(fecha_minima, fecha_maxima)  # Valor inicial = rango completo
)

print("Widgets definidos:")
print(f"- Selector de variable: {selector_variable.name} con {len(selector_variable.options)} opciones")
print(f"- Selector de rango de fechas: {selector_rango_fechas.name} de {selector_rango_fechas.start.date()} a {selector_rango_fechas.end.date()}")

"""## Crear visualizaciones

### Subtask:
Generar las visualizaciones (gráficos de series de tiempo, etc.) que se actualizarán según la selección del usuario en los widgets.

"""

import holoviews as hv
import hvplot.pandas  # noqa
import pandas as pd # Importar pandas para la regresión lineal

def crear_grafico_serie_tiempo(variable, rango_fechas):
    """
    Genera un gráfico interactivo de serie de tiempo para una variable
    seleccionada dentro de un rango de fechas específico, incluyendo una línea de tendencia.

    Parámetros:
        variable (str): Nombre de la columna (variable) a graficar.
        rango_fechas (tuple): Tupla con la fecha inicial y final para filtrar el DataFrame.

    Retorna:
        hvplot.hvPlot: Gráfico interactivo de la serie de tiempo con línea de tendencia.
    """
    fecha_inicio, fecha_fin = rango_fechas

    # Filtrar el DataFrame por el rango de fechas seleccionado
    # Usamos .loc ya que el índice del DataFrame es datetime
    df_filtrado = cartagena_df.loc[fecha_inicio:fecha_fin].copy() # Usar .copy() para evitar SettingWithCopyWarning

    if df_filtrado.empty:
        print(f"No se encontraron datos para {variable} en el rango de fechas seleccionado.")
        # Retorna un objeto vacío de HoloViews si no hay datos
        return hv.Overlay()

    # Crear el gráfico de serie de tiempo con hvplot
    # El índice (valid_time) se usa automáticamente como eje X
    grafico = df_filtrado.hvplot(
        y=variable,
        title=f'Serie de tiempo de {variable} con Tendencia ({fecha_inicio.strftime("%Y-%m-%d")} a {fecha_fin.strftime("%Y-%m-%d")})',
        xlabel='Tiempo',
        ylabel=variable,
        tools=['hover', 'pan', 'wheel_zoom', 'box_zoom', 'reset', 'save'] # Agregar herramientas
    )

    # Agregar línea de tendencia
    # Para la línea de tendencia, necesitamos una representación numérica del tiempo
    df_filtrado['time_ordinal'] = pd.to_datetime(df_filtrado.index).map(pd.Timestamp.toordinal)

    # Calcular la regresión lineal
    try:
        # Asegurarse de que no haya NaN en los datos para la regresión
        df_regresion = df_filtrado[[variable, 'time_ordinal']].dropna()
        if not df_regresion.empty:
            m, b = np.polyfit(df_regresion['time_ordinal'], df_regresion[variable], 1)
            df_filtrado['tendencia'] = m * df_filtrado['time_ordinal'] + b

            # Crear el gráfico de la línea de tendencia
            tendencia_grafico = df_filtrado.hvplot(
                y='tendencia',
                color='red',
                line_dash='dashed',
                label='Tendencia',
                tools=['hover', 'pan', 'wheel_zoom', 'box_zoom', 'reset', 'save']
            )
            # Combinar el gráfico original y la línea de tendencia
            grafico = grafico * tendencia_grafico
        else:
             print(f"No hay datos suficientes para calcular la tendencia para {variable} en el rango de fechas seleccionado.")
    except Exception as e:
        print(f"No se pudo calcular la tendencia para {variable}: {e}")


    return grafico

print("Función 'crear_grafico_serie_tiempo' definida.")

"""## Combinar widgets y visualizaciones en un dashboard

### Subtask:
Usar la biblioteca de dashboarding seleccionada (`panel`, etc.) para organizar los widgets y las visualizaciones en un diseño coherente de dashboard.

"""

# --- Crear el dashboard interactivo con Panel ---
import panel as pn

pn.extension()

# Vincular los widgets al gráfico
grafico_interactivo = pn.bind(crear_grafico_serie_tiempo, variable=selector_variable, rango_fechas=selector_rango_fechas)

# Diseño del dashboard
dashboard = pn.Column(
    "# Dashboard Climático para Cartagena",
    pn.Row(selector_variable, selector_rango_fechas),
    grafico_interactivo
)

# Mostrar el dashboard
dashboard

"""- valid_time: La marca de tiempo de la observación o predicción. Indica la fecha y hora a la que se refieren los datos.

- latitude: La latitud del punto de la cuadrícula.
- longitude: La longitud del punto de la cuadrícula.
- tp (Total Precipitation): Precipitación total acumulada. A menudo se mide en metros (m).

- ssrd (Surface Short-wave Radiation Downwards): Radiación solar de onda corta (visible y ultravioleta) que llega a la superficie de la Tierra. Se mide en Julios por metro cuadrado (J/m²).

- strd (Surface Thermal Radiation Downwards): Radiación térmica de onda larga (infrarroja) que llega a la superficie de la Tierra desde la atmósfera. Se mide en Julios por metro cuadrado (J/m²).

- ssr (Surface Net Short-wave Radiation): Balance de radiación solar de onda corta en la superficie (entrante menos saliente). Se mide en Julios por metro cuadrado (J/m²).

- str (Surface Net Thermal Radiation): Balance de radiación térmica de onda larga en la superficie (entrante menos saliente). Se mide en Julios por metro cuadrado (J/m²).

- uvb (Total UV-B radiation in the atmosphere): Radiación ultravioleta-B total en la atmósfera. Se mide en Julios por metro cuadrado (J/m²).
e (Evaporation): Tasa de evaporación. A menudo se mide en metros de agua evaporada (m).

- ro (Runoff): Escorrentía (cantidad de agua que fluye sobre la superficie terrestre). A menudo se mide en metros (m).

- sf (Snowfall): Caída de nieve. A menudo se mide en metros de equivalente de agua (m).

- u10 (10 metre U wind component): Componente zonal (este-oeste) del viento a 10 metros sobre la superficie. Se mide en metros por segundo (m/s).

- v10 (10 metre V wind component): Componente meridional (norte-sur) del viento a 10 metros sobre la superficie. Se mide en metros por segundo (m/s).

- d2m (2 metre dewpoint temperature): Temperatura del punto de rocío a 2 metros sobre la superficie. Se mide en Kelvin (K).

- t2m (2 metre temperature): Temperatura del aire a 2 metros sobre la superficie. Se mide en Kelvin (K).

- msl (Mean sea level pressure): Presión a nivel medio del mar. Se mide en Pascales (Pa).

- sst (Sea surface temperature): Temperatura de la superficie del mar. Se mide en Kelvin (K).

- sp (Surface pressure): Presión en la superficie. Se mide en Pascales (Pa).

- skt (Skin temperature): Temperatura de la "piel" o superficie (puede ser tierra, agua, hielo, etc.). Se mide en Kelvin (K).

- tcc (Total cloud cover): Cobertura total de nubes. Es una fracción entre 0 y 1.

- si10 (10 metre wind speed): Velocidad del viento a 10 metros sobre la superficie. Se mide en metros por segundo (m/s).

- sd (Snow depth): Profundidad de la nieve. Se mide en metros (m).

- stl1 (Soil temperature level 1): Temperatura del suelo en el primer nivel de profundidad. Se mide en Kelvin (K).

- swvl1 (Volumetric soil water layer 1): Contenido de agua volumétrico en la primera capa del suelo. Es una fracción entre 0 y 1.

- cl, dl (Low cloud cover, High cloud cover): Cobertura de nubes bajas y altas. Son fracciones entre 0 y 1.

- mwd (Mean wave direction): Dirección media de las olas. Se mide en grados (°).

- mwp (Mean wave period): Período medio de las olas. Se mide en segundos (s).

- swh (Significant wave height): Altura significativa de las olas. Se mide en metros (m).
- number: Índice de miembro del conjunto (si es un conjunto de datos de pronóstico por conjunto).

- expver: Versión experimental del reanálisis o pronóstico.
"""

# Exportar el dashboard como un archivo HTML independiente
dashboard.save('cartagena_climate_dashboard.html', embed=True)
print("Dashboard exported as 'cartagena_climate_dashboard.html'")



def crear_box_plot(variable, rango_fechas):
    """
    Genera un box plot interactivo para una variable seleccionada,
    agrupado por mes, dentro de un rango de fechas específico.

    Parámetros:
        variable (str): Nombre de la columna (variable) a graficar.
        rango_fechas (tuple): Tupla con la fecha inicial y final para filtrar el DataFrame.

    Retorna:
        hvplot.hvPlot: Gráfico interactivo de box plot.
    """
    fecha_inicio, fecha_fin = rango_fechas

    # Filtrar el DataFrame por el rango de fechas seleccionado
    df_filtrado = cartagena_df.loc[fecha_inicio:fecha_fin].copy() # Usar .copy() para evitar SettingWithCopyWarning

    if df_filtrado.empty:
        print(f"No se encontraron datos para {variable} en el rango de fechas seleccionado.")
        return hv.Overlay()

    # Extraer el mes para agrupar
    df_filtrado['month'] = df_filtrado.index.month

    # Crear el box plot con hvplot
    box_plot = df_filtrado.hvplot.box(
        y=variable,
        by='month',
        title=f'Box Plot de {variable} por mes ({fecha_inicio.strftime("%Y-%m-%d")} a {fecha_fin.strftime("%Y-%m-%d")})',
        xlabel='Mes',
        ylabel=variable,
        rot=90 # Rotar etiquetas del eje x si es necesario
    )

    return box_plot

print("Función 'crear_box_plot' definida.")

# --- Crear los gráficos interactivos con Panel ---
# Vinculamos los widgets a las funciones para que los gráficos se actualicen
grafico_interactivo = pn.bind(crear_grafico_serie_tiempo, variable=selector_variable, rango_fechas=selector_rango_fechas)
box_plot_interactivo = pn.bind(crear_box_plot, variable=selector_variable, rango_fechas=selector_rango_fechas)

print("Gráficos interactivos creados y vinculados a los widgets.")

# --- Crear el diseño del dashboard con Panel ---
# Organizamos los widgets y los gráficos en un diseño de columna y fila
dashboard_final = pn.Column(
    "# Dashboard Climático para Cartagena",
    pn.Row(selector_variable, selector_rango_fechas), # Widgets en una fila superior
    pn.Row(grafico_interactivo, box_plot_interactivo) # Gráficos en una fila inferior
)

# Mostrar el dashboard
dashboard_final

"""# Descomposicion de la serie"""

# Lista de variables a descomponer
variables_a_descomponer = [
    't2m',  # 2 metre temperature
    'tp',   # Total Precipitation
    'ro',   # Runoff
    'e',    # Evaporation
    'tcc',  # Total Cloud Cover
    'sst',  # Sea surface temperature
    'uvb',  # Total UV-B radiation
    'ssrd', # Surface Short-wave Radiation Downwards
    'si10', # 10 metre wind speed
    'msl'   # Mean sea level pressure
]

# Asegurarse de que el DataFrame tiene un índice de tiempo
if not isinstance(cartagena_df.index, pd.DatetimeIndex):
    print("El DataFrame no tiene un índice de tiempo. Convirtiendo 'valid_time' a índice.")
    cartagena_df['valid_time'] = pd.to_datetime(cartagena_df['valid_time'])
    cartagena_df = cartagena_df.set_index('valid_time')
    # Asegurarse de que el índice esté ordenado para la descomposición
    cartagena_df = cartagena_df.sort_index()


# Realizar la descomposición estacional para cada variable
descomposiciones = {}
# El periodo de descomposición de 20 años para datos horarios es 20 * 365.25 * 24 horas
# Usar un periodo tan grande directamente en seasonal_decompose puede ser problemático.
# Si el objetivo es la tendencia a largo plazo y estacionalidad anual, resamplear a diario o mensual
# y usar period=365 o period=12 (o 20 años * freq) sería más manejable.
# Sin embargo, el usuario pidió period=20. Si esto se refiere a 20 pasos de la serie (ej: 20 horas), es un ciclo muy corto.
# Reinterpretaré el "periodo de 20 años" como la duración total de la tendencia que se quiere observar,
# y usaré un período adecuado para la estacionalidad anual en datos horarios, que es ~8766 horas.
# Si el usuario realmente quiere un período de 20 "algo", necesitaríamos aclarar la frecuencia de ese "algo".
# Basado en el intento anterior y el error, parece que la frecuencia horaria es lo que se está usando.
# Un período de 20 para datos horarios no mostrará una estacionalidad anual o de 20 años.
# Si el usuario quiere una estacionalidad anual, el período debería ser 24 * 365.25 ~ 8766.
# Si quiere una tendencia de 20 años, seasonal_decompose con un periodo tan grande es inusual y propenso a errores.
# Vamos a usar un período que capture la estacionalidad anual en datos horarios (8766).
# O si el usuario insiste en period=20, lo usamos pero aclaramos que es un período corto.
# Dada la ambigüedad y el error anterior, el problema principal es NaNs y frecuencia irregular.
# Priorizaré asegurar la frecuencia regular y luego usar un período más plausible para estacionalidad (ej: 24 para diario, 8766 para anual).

# El error "This function does not handle missing values" persiste incluso después de fillna/interpolate.
# Esto puede ser porque `seasonal_decompose` necesita un índice con una frecuencia *inferida* correctamente Y sin NaNs.
# Asegurarse de que el índice tiene una frecuencia regular ANTES de la descomposición es crucial.
# Resamplear o usar `asfreq` *después* de rellenar NaNs puede ayudar a solidificar la frecuencia.

periodo_estacional = 24 # Periodo típico para ciclo diario en datos horarios. Si se quiere estacionalidad anual, usar 24*365.25 (~8766)

print(f"Realizando descomposición estacional con un período de {periodo_estacional} horas para las siguientes variables:")
for variable in variables_a_descomponer:
    if variable in cartagena_df.columns:
        print(f"- {variable}")
        try:
            # seasonal_decompose requiere que no haya NaNs en la serie
            # Asegurarse de que no hay NaNs remanentes después del preprocesamiento
            serie_sin_nan = cartagena_df[variable].dropna()

            # Asegurarse de que la serie tiene una frecuencia regular *antes* de la descomposición
            # Esto puede requerir resamplear o usar asfreq con fillna
            if serie_sin_nan.empty:
                 print(f"  No hay datos válidos para la descomposición de '{variable}'. Saltando.")
                 descomposiciones[variable] = None
                 continue

            # Usar asfreq('H') para asegurar frecuencia horaria explícita después de rellenar NaNs
            # Rellenar los huecos que asfreq pueda crear (aunque fillna/bfill ya se hicieron)
            serie_regular_freq = cartagena_df[variable].asfreq('H').fillna(method='ffill').fillna(method='bfill')

            # Verificar si hay NaNs después de asfreq y fillna
            if serie_regular_freq.isnull().sum() > 0:
                print(f"  Advertencia: Quedan NaNs en la serie '{variable}' después de asfreq y fillna. No se puede descomponer.")
                descomposiciones[variable] = None
                continue


            descomposicion = seasonal_decompose(serie_regular_freq, model='additive', period=periodo_estacional)
            descomposiciones[variable] = descomposicion

            # Opcional: Mostrar los componentes de la descomposición para cada variable
            print(f"  Descomposición para '{variable}' completada. Componentes:")
            display(descomposicion.plot()) # Muestra el gráfico de descomposición
            plt.show()

        except Exception as e:
            print(f"  Error al descomponer '{variable}': {e}")
            descomposiciones[variable] = None
    else:
        print(f"  La variable '{variable}' no se encuentra en el DataFrame.")
        descomposiciones[variable] = None

print("\nProceso de descomposición completado.")

from statsmodels.tsa.stattools import adfuller

# Lista de variables para realizar la prueba ADF
variables_para_adf = [
    't2m',  # 2 metre temperature
    'tp',   # Total Precipitation
    'ro',   # Runoff
    'e',    # Evaporation
    'tcc',  # Total Cloud Cover
    'sst',  # Sea surface temperature
    'uvb',  # Total UV-B radiation
    'ssrd', # Surface Short-wave Radiation Downwards
    'si10', # 10 metre wind speed
    'msl'   # Mean sea level pressure
]

print("Realizando la prueba de Dickey-Fuller Aumentada (ADF) para las siguientes variables:")

for variable in variables_para_adf:
    if variable in cartagena_df.columns:
        print(f"\n--- Resultados de la prueba ADF para '{variable}' ---")
        try:
            # La prueba ADF también es sensible a los valores NaN, aunque en teoría
            # el preprocesamiento anterior debería haberlos manejado.
            # Aseguramos que la serie no tenga NaNs antes de pasarla a adfuller
            serie_sin_nan = cartagena_df[variable].dropna()

            if serie_sin_nan.empty:
                print(f"  Advertencia: La serie '{variable}' está vacía después de eliminar NaNs. No se puede realizar la prueba ADF.")
                continue

            # Realizar la prueba ADF
            resultado = adfuller(serie_sin_nan)

            valor_adf = resultado[0]
            valor_p = resultado[1]
            valor_criticos = resultado[4]

            print(f"  ADF Statistic: {valor_adf}")
            print(f"  p-value: {valor_p}")
            print("  Critical Values:")
            for clave, valor in valor_criticos.items():
                print(f"     {clave}: {valor}")

            # Interpretar el resultado (opcional)
            if valor_p <= 0.05:
                print("  Conclusión: La serie de tiempo es probablemente estacionaria (rechazamos la hipótesis nula).")
            else:
                print("  Conclusión: La serie de tiempo es probablemente no estacionaria (no podemos rechazar la hipótesis nula).")

        except Exception as e:
            print(f"  Error al realizar la prueba ADF para '{variable}': {e}")
    else:
        print(f"\n--- '{variable}' no se encuentra en el DataFrame. Saltando. ---")

print("\nProceso de prueba ADF completado para todas las variables solicitadas.")

"""# Modelo de Regresión con Variables Instrumentales (IV) para Análisis Climático

## 1️ Predicción del Clima Diario

El modelo IV busca estimar la **temperatura media del día siguiente** (`Temp_{t+1}`) usando variables climáticas observadas y corregir posibles problemas de endogeneidad (por ejemplo, cuando la radiación o la humedad afectan y son afectadas por la temperatura).

### Ecuación estructural:

\[
Temp_{t+1} = \alpha + \beta_1 \, Temp_{t} + \beta_2 \, d2m_{t} + \beta_3 \, SSR_{t} + \beta_4 \, STR_{t} + \beta_5 \, P_{t} + \varepsilon_{t}
\]

donde:  
- \( Temp_{t+1} \): Temperatura media del día siguiente (variable dependiente)  
- \( Temp_t \): Temperatura actual a 2m (`2m_temperature`)  
- \( d2m_t \): Temperatura del punto de rocío (`2m_dewpoint_temperature`)  
- \( SSR_t \): Radiación solar neta (`surface_net_solar_radiation`)  
- \( STR_t \): Radiación térmica neta (`surface_net_thermal_radiation`)  
- \( P_t \): Precipitación total (`total_precipitation`)  
- \( \varepsilon_t \): Término de error (choques climáticos no observados)

### Ecuación de primer paso (instrumental):
Algunas de las variables explicativas pueden ser endógenas. Por ejemplo, la radiación o la humedad pueden estar correlacionadas con choques no observados en el clima.  
Se instrumentan con variables exógenas, como las condiciones del suelo o el viento:

\[
SSR_t = \gamma_0 + \gamma_1 \, soil\_temperature\_level\_1 + \gamma_2 \, surface\_pressure + \gamma_3 \, wind\_speed_{10m} + \nu_t
\]

\[
STR_t = \delta_0 + \delta_1 \, lake\_total\_depth + \delta_2 \, soil\_type + \delta_3 \, surface\_solar\_radiation\_downwards + \eta_t
\]

donde:
- `wind_speed_10m` = sqrt(`10m_u_component_of_wind`² + `10m_v_component_of_wind`²)

El modelo final combina ambos pasos usando **Two-Stage Least Squares (2SLS)** para obtener estimaciones insesgadas de los coeficientes \(\beta_i\).

---

## 2️ Modelo IV para Riesgo de Desastres Climáticos

Para modelar la **probabilidad de un evento climático extremo** (por ejemplo, inundaciones o sequías), se define una variable binaria:

\[
Disaster_t =
\begin{cases}
1, & \text{si ocurre un evento extremo (ej. precipitación o temperatura fuera del percentil 95)} \\
0, & \text{en caso contrario}
\end{cases}
\]

### Ecuación estructural (modelo de probabilidad lineal o probit IV):
\[
Pr(Disaster_t = 1) = \Phi(\alpha + \beta_1 \, Temp_{t} + \beta_2 \, P_t + \beta_3 \, STR_t + \beta_4 \, SSR_t + \beta_5 \, Runoff_t + u_t)
\]

donde:
- \( \Phi \): Función de distribución acumulada (en caso de usar Probit)
- \( Runoff_t \): Escorrentía total (`total_runoff`), importante para inundaciones
- Las demás variables son las mismas que en el modelo anterior

### Variables instrumentales propuestas:
Se emplean variables menos afectadas por choques climáticos inmediatos:
\[
\{ soil\_temperature\_level\_1, soil\_water\_layer\_1, lake\_cover, leaf\_area\_index\_high\_vegetation, wind\_speed_{10m} \}
\]

Estas capturan condiciones estructurales del territorio que influyen indirectamente en el riesgo de desastres, pero no son determinadas por el evento mismo.

---

## 3️ Objetivo del modelo

Ambos modelos permiten:
- Estimar tendencias de temperatura y precipitación a corto y mediano plazo.
- Evaluar cómo variables estructurales del suelo y vegetación actúan como **instrumentos** para mejorar la predicción.
- Proveer un **coeficiente de confianza** sobre la probabilidad de eventos extremos para cada región del país.

---

"""

# -----------------------
# Modelo Probit IV de Dos Etapas para la Probabilidad de Lluvia Mañana
# (Implementado en una sola celda)
# -----------------------

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.discrete.discrete_model import Probit
import pickle

# Asumiendo que 'cartagena_df' ya está cargado y preprocesado.
# Asegúrese de que el índice sea un DatetimeIndex y que los datos estén limpios.

# -----------------------
# 1) Definir la variable dependiente binaria: Probabilidad de Lluvia Mañana
# -----------------------
# Definir 'Rain_Tomorrow' como 1 si la precipitación total (tp) es mayor que un pequeño umbral (ej. 0) en el día siguiente, 0 en caso contrario.
# Necesitamos crear una versión rezagada de 'tp' para representar la precipitación del día siguiente.
cartagena_df['tp_next'] = cartagena_df['tp'].shift(-1)

# Crear la variable binaria 'Rain_Tomorrow' basada en 'tp_next'
# Usando un pequeño umbral (ej. 0.0001 m) para distinguir entre la ausencia de lluvia y cantidades mínimas.
rain_threshold = 0.0001 # Puede ajustar este umbral
cartagena_df['Rain_Tomorrow'] = (cartagena_df['tp_next'] > rain_threshold).astype(int)

print(f"Variable 'Rain_Tomorrow' creada (umbral > {rain_threshold} m). Conteo de eventos de lluvia mañana: {cartagena_df['Rain_Tomorrow'].sum()}")


# -----------------------
# 2) Definir variables para el enfoque de dos etapas
# -----------------------
# Queremos modelar: Pr(Rain_Tomorrow = 1) = Φ(α + β1 Vars_Exog + β2 Var_Endog_Predicha + u_t)

# Variable dependiente para el modelo Probit
dep_probit = 'Rain_Tomorrow'

# Variables exógenas (asumidas no correlacionadas con el término de error en la segunda etapa)
# Usemos 't2m' (temperatura a 2m) y 'msl' (presión media a nivel del mar) de hoy como predictores exógenos.
# Añadir un término constante.
exog_vars_probit = ['t2m', 'msl']
exog_probit = sm.add_constant(cartagena_df[exog_vars_probit], has_constant='add')

# Variable endógena (potencialmente correlacionada con el término de error)
# La precipitación total de hoy ('tp') es probable que sea endógena al predecir la lluvia de mañana.
endog_vars_probit = ['tp']

# Instrumentos (exógenos y correlacionados con la variable endógena, pero no con el término de error)
# Los instrumentos deben estar correlacionados con la precipitación de hoy ('tp') pero no directamente con la probabilidad de lluvia de mañana, excepto a través de la precipitación de hoy.
# Instrumentos plausibles podrían incluir humedad del suelo ('swvl1'), temperatura del suelo ('stl1'), velocidad del viento ('si10'), radiación ('ssrd', 'strd').
# Necesitamos al menos tantos instrumentos como variables endógenas. Tenemos 1 variable endógena ('tp'), así que necesitamos al menos 1 instrumento. Usemos unos cuantos.
instrument_vars = ['swvl1', 'stl1', 'si10']
instruments = cartagena_df[instrument_vars]

# Combinar variables exógenas e instrumentos para los predictores de la primera etapa
first_stage_predictors = sm.add_constant(pd.concat([cartagena_df[exog_vars_probit], instruments], axis=1), has_constant='add')

# Eliminar filas con NaNs necesarios antes de ajustar los modelos
required_cols_probit = [dep_probit] + exog_vars_probit + endog_vars_probit + instrument_vars + ['tp_next'] # Incluir tp_next para Rain_Tomorrow
df_model_probit_2step = cartagena_df.dropna(subset=required_cols_probit).copy()

# Re-definir matrices con el DataFrame limpio
exog_probit_clean_2step = sm.add_constant(df_model_probit_2step[exog_vars_probit], has_constant='add')
endog_probit_clean_2step = df_model_probit_2step[endog_vars_probit]
instruments_clean_2step = df_model_probit_2step[instrument_vars]
first_stage_predictors_clean_2step = sm.add_constant(pd.concat([df_model_probit_2step[exog_vars_probit], instruments_clean_2step], axis=1), has_constant='add')
y_probit_clean_2step = df_model_probit_2step[dep_probit]

print(f"\nRegistros usados en modelo Probit IV (Dos Etapas): {len(df_model_probit_2step)}")

# -----------------------
# 3) Primera Etapa: Regresar la variable endógena ('tp') sobre todos los instrumentos y variables exógenas (OLS)
# -----------------------
print("\n--- Regresión de la Primera Etapa (OLS para tp) ---")
try:
    # Asegurarse de que la variable dependiente para la primera etapa no tenga NaNs (ya hecho por dropna anterior)
    y_fs = df_model_probit_2step['tp']
    X_fs = first_stage_predictors_clean_2step

    if y_fs.empty or X_fs.empty:
          print(f"  Advertencia: No hay suficientes datos para la regresión de la primera etapa de 'tp'. Saltando.")
          predicted_tp = None
    else:
        model_fs = sm.OLS(y_fs, X_fs).fit()
        predicted_tp = model_fs.predict(first_stage_predictors_clean_2step) # Predecir para todas las filas en los datos limpios
        print(model_fs.summary()) # Imprimir resumen de la primera etapa
        print(f"\nR-cuadrado de la Primera Etapa de 'tp': {model_fs.rsquared:.4f}")

except Exception as e:
    print(f"  Error durante la regresión de la primera etapa para 'tp': {e}")
    predicted_tp = None


# -----------------------
# 4) Segunda Etapa: Regresión Probit usando la variable endógena predicha y variables exógenas
# -----------------------
print("\n--- Regresión de la Segunda Etapa (Probit para Rain_Tomorrow) ---")

if predicted_tp is None:
    print("No se puede ejecutar el modelo Probit de la segunda etapa porque la primera etapa falló.")
    probit_2step_results = None
else:
    # Combinar variables exógenas y la variable endógena predicha para los predictores de la segunda etapa
    second_stage_predictors = pd.concat([exog_probit_clean_2step, predicted_tp.rename('tp_predicted')], axis=1)

    # Asegurarse de que no haya NaNs en los predictores de la segunda etapa o en la variable dependiente después de la concatenación/predicción
    # (dropna en df_model_probit_2step y la predicción en su índice deberían manejar esto)
    # Verificar la forma solo por si acaso
    if second_stage_predictors.shape[0] != y_probit_clean_2step.shape[0]:
          print("Desajuste entre las formas de los predictores de la segunda etapa y la variable dependiente después de la primera etapa. No se puede ejecutar Probit.")
          probit_2step_results = None
    else:
        y_ss = y_probit_clean_2step
        X_ss = second_stage_predictors

        try:
            # Ajustar el modelo Probit
            probit_model_ss = Probit(y_ss, X_ss)
            probit_2step_results = probit_model_ss.fit()

            # Imprimir el resumen del modelo Probit de la segunda etapa
            print(probit_2step_results.summary())

            # Guardar los resultados del modelo Probit de la segunda etapa
            with open('probit_2step_rain_tomorrow.pkl', 'wb') as f:
                 pickle.dump(probit_2step_results, f)
            print("\nResultados del modelo Probit de Dos Etapas guardados en 'probit_2step_rain_tomorrow.pkl'")

        except Exception as e:
            print(f"Error al ajustar el modelo Probit de la segunda etapa: {e}")
            probit_2step_results = None

print("\nProceso Probit IV de Dos Etapas completado para predecir lluvia mañana.")

"""## Interpretación de los Resultados del Modelo IV Probit para Predecir Lluvia Mañana

Basado en los resultados obtenidos del modelo de Regresión IV Probit de dos etapas (implementado en la celda anterior `L-mA11wjyr_l`), podemos interpretar lo siguiente:

### 1. Resultados de la Primera Etapa (OLS para `tp` - Precipitación Total de Hoy)

Esta regresión OLS predice la precipitación total observada hoy (`tp`) utilizando las variables exógenas (`t2m`, `msl`) y los instrumentos (`swvl1`, `stl1`, `si10`).

*   **R-squared (0.436):** Indica que aproximadamente el 43.6% de la variabilidad en la precipitación total de hoy (`tp`) es explicada por las variables incluidas en la primera etapa. Este valor es razonable y sugiere que los instrumentos y las variables exógenas tienen poder predictivo sobre la variable endógena.
*   **Prob (F-statistic) (0.00):** El p-valor del estadístico F es muy bajo (esencialmente 0), lo que indica que el modelo de la primera etapa en su conjunto es estadísticamente significativo; al menos una de las variables explicativas (exógenas o instrumentos) tiene un efecto no nulo sobre `tp`.
*   **Coeficientes de los Instrumentos (`swvl1`, `stl1`, `si10`):** Debemos observar si los instrumentos son estadísticamente significativos (P>|t| bajo) y tienen un efecto plausible sobre `tp`.
    *   `swvl1` (contenido de agua en el suelo): Coeficiente positivo y altamente significativo (P>|t| = 0.000, t = 35.082). Esto tiene sentido, ya que mayor humedad en el suelo hoy podría estar relacionada con eventos de precipitación recientes o condiciones que favorecen la precipitación.
    *   `stl1` (temperatura del suelo): Coeficiente positivo y significativo (P>|t| = 0.005, t = 2.811). La temperatura del suelo también puede influir en los procesos atmosféricos que llevan a la precipitación.
    *   `si10` (velocidad del viento a 10m): Coeficiente positivo, pero marginalmente significativo (P>|t| = 0.067, t = 1.831). La velocidad del viento puede influir en el transporte de humedad y la formación de nubes.
*   **Coeficientes de las Variables Exógenas (`t2m`, `msl`):**
    *   `t2m` (temperatura a 2m): Coeficiente positivo y significativo (P>|t| = 0.000, t = 4.259). Temperaturas más altas hoy podrían estar asociadas con mayor evaporación y potencial de precipitación.
    *   `msl` (presión a nivel del mar): Coeficiente negativo y altamente significativo (P>|t| = 0.000, t = -12.251). Presiones más bajas suelen asociarse con sistemas de baja presión que traen consigo precipitación.

*   **Cond. No. (9.74e+07):** El número de condición es muy alto. Esto sugiere que puede haber **multicolinealidad** en los predictores de la primera etapa (variables exógenas e instrumentos). Aunque la regresión OLS se puede estimar con multicolinealidad, puede hacer que las estimaciones de los coeficientes individuales sean inestables y difíciles de interpretar. Sin embargo, el objetivo principal de la primera etapa en un modelo IV es obtener una buena predicción de la variable endógena, y un R-squared decente puede ser suficiente incluso con multicolinealidad si los instrumentos son fuertes.

### 2. Resultados de la Segunda Etapa (Probit para `Rain_Tomorrow` - Probabilidad de Lluvia Mañana)

Esta regresión Probit modela la probabilidad de que llueva mañana (`Rain_Tomorrow`) utilizando las variables exógenas (`t2m`, `msl`) y la **precipitación total de hoy *predicha*** (`tp_predicted`) obtenida de la primera etapa.

*   **No. Observations (6798):** El número de observaciones utilizadas en el modelo.
*   **Pseudo R-squ. (0.5563):** Es una medida análoga al R-squared en modelos lineales para modelos de respuesta binaria. Un valor de 0.5563 sugiere que el modelo explica una parte considerable de la variabilidad en la probabilidad de lluvia mañana.
*   **LLR p-value (0.000):** El p-valor del test de Ratio de Verosimilitud (Likelihood Ratio Test) es muy bajo, indicando que el modelo en su conjunto es estadísticamente significativo y predice la probabilidad de lluvia mañana mejor que un modelo sin predictores.
*   **Coeficientes (interpretación en términos de cambio en el *índice latente* o *z-score*):** En un modelo Probit, los coeficientes indican el cambio en el índice latente (la variable continua subyacente que, al cruzar un umbral, determina el resultado binario) asociado con un cambio de una unidad en el predictor, manteniendo las demás variables constantes. Un coeficiente positivo aumenta la probabilidad del evento (lluvia), y uno negativo la disminuye. Para interpretar el efecto en la probabilidad en sí, se necesitarían los efectos marginales.
    *   **`const` (-315.3725):** La constante del modelo base.
    *   **`t2m` (0.1261):** Coeficiente positivo y altamente significativo (P>|z| = 0.000). Mayor temperatura hoy está asociada con una mayor probabilidad de lluvia mañana.
    *   **`msl` (0.0031):** Coeficiente positivo y altamente significativo (P>|z| = 0.000). Mayor presión a nivel del mar hoy está asociada con una mayor probabilidad de lluvia mañana. *Nota: Esto es un poco contraintuitivo, ya que menor presión suele asociarse con lluvia. Podría valer la pena revisar la variable `msl` o considerar transformaciones o interacciones.*
    *   **`tp_predicted` (1.314e+04):** Coeficiente positivo y altamente significativo (P>|z| = 0.000). Una mayor precipitación *predicha* para hoy (basada en los instrumentos y exógenas) está fuertemente asociada con una mayor probabilidad de lluvia mañana. Este es un resultado clave que apoya la idea de que la precipitación actual (instrumentada) es un predictor importante de la lluvia futura.

### Conclusión General

El modelo de Regresión IV Probit de dos etapas parece ser efectivo para predecir la probabilidad de lluvia en Cartagena al día siguiente. La primera etapa muestra que los instrumentos seleccionados (`swvl1`, `stl1`, `si10`) son relevantes para predecir la precipitación actual, aunque la multicolinealidad en los predictores de la primera etapa es una advertencia. La segunda etapa Probit indica que la temperatura actual, la presión a nivel del mar y, crucialmente, la precipitación actual *instrumentada* son predictores significativos de la probabilidad de lluvia mañana.

# **Prediccion de lluvia con IV**
"""

# --- Predecir Lluvia Mañana usando el Modelo Probit IV ajustado ---

import pandas as pd
import numpy as np
import statsmodels.api as sm
import pickle

# Cargar los resultados del modelo Probit ajustado
try:
    with open('probit_2step_rain_tomorrow.pkl', 'rb') as f:
        probit_2step_results = pickle.load(f)
    print("Modelo Probit cargado exitosamente.")

except FileNotFoundError:
    print("Error: No se encontró el archivo del modelo 'probit_2step_rain_tomorrow.pkl'.")
    print("Por favor, asegúrese de que la celda anterior (L-mA11wjyr_l) se ejecutó correctamente para guardar el modelo.")
    probit_2step_results = None
except Exception as e:
    print(f"Ocurrió un error al cargar el modelo: {e}")
    probit_2step_results = None


if probit_2step_results is not None:
    # Necesitamos las variables predictoras para el *último* punto de datos disponible
    # en el 'cartagena_df' original para predecir la probabilidad del día siguiente.

    # Asegurarse de que 'cartagena_df' esté disponible y tenga las columnas necesarias
    if 'cartagena_df' not in locals() and 'cartagena_df' not in globals():
          print("Error: No se encontró el DataFrame 'cartagena_df'.")
          print("Por favor, asegúrese de que se ejecutaron los pasos de carga y preprocesamiento de datos.")
    else:
        # Obtener la última fila del DataFrame original
        # Tenga cuidado con los NaNs en la última fila si 'tp_next' se calculó en el df completo
        # Usemos la última fila del dataframe *limpio* usado para el modelado si es posible,
        # o reconstruir los predictores para la última marca de tiempo en 'cartagena_df'.

        # Usando el último punto de datos válido del dataframe usado para el ajuste
        # Esto asegura que tenemos todas las columnas requeridas sin NaNs
        last_data_point = df_model_probit_2step.iloc[[-1]].copy()

        # Reconstruir las variables exógenas y la variable endógena predicha para este punto
        # Predecir 'tp' para este último punto de datos usando el modelo de la primera etapa
        # Necesitamos los resultados del modelo de la primera etapa o sus predictores y coeficientes
        # Dado que guardamos los resultados de la *segunda etapa*, necesitamos volver a predecir 'tp' usando la lógica de la primera etapa.
        # El modelo de la primera etapa requiere las variables exógenas e instrumentales para el último punto.

        # Predictores para la primera etapa para el último punto de datos
        # Asegúrese de que el DataFrame 'first_stage_predictors_clean_2step' todavía esté disponible
        if 'first_stage_predictors_clean_2step' not in locals() and 'first_stage_predictors_clean_2step' not in globals():
             print("Error: No se encontró el DataFrame 'first_stage_predictors_clean_2step'.")
             print("No se puede predecir la variable endógena para el último punto de datos.")
        else:
              # Obtener la fila correspondiente de los predictores de la primera etapa
              last_first_stage_predictors = first_stage_predictors_clean_2step.iloc[[-1]].copy()

              # Necesitamos los coeficientes del *modelo OLS de la primera etapa* para predecir 'tp'.
              # El modelo de la primera etapa fue ajustado dentro de la celda anterior (L-mA11wjyr_l)
              # y los resultados ('model_fs') no fueron guardados o devueltos explícitamente.
              # Para hacer una predicción, necesitaríamos reajustar la primera etapa o guardar sus resultados.

              # Como solución provisional para la demostración, si 'model_fs' todavía está en el entorno
              # de una ejecución reciente de L-mA11wjyr_l, podemos usarlo.
              # Un enfoque más robusto sería guardar también el modelo de la primera etapa.

              # Verificar si 'model_fs' está disponible (menos confiable en sesión interactiva)
              if 'model_fs' in locals() or 'model_fs' in globals():
                  print("Usando 'model_fs' disponible para la predicción de la primera etapa.")
                  predicted_tp_last = model_fs.predict(last_first_stage_predictors)

                  # Reconstruir los predictores de la segunda etapa para el último punto de datos
                  last_exog_probit_clean_2step = exog_probit_clean_2step.iloc[[-1]].copy()
                  last_second_stage_predictors = pd.concat([last_exog_probit_clean_2step, predicted_tp_last.rename('tp_predicted')], axis=1)


                  # Asegurarse de que los nombres de las columnas coincidan exactamente con los datos de entrenamiento del modelo
                  last_second_stage_predictors.columns = probit_2step_results.params.index

                  # Predecir la probabilidad usando el modelo Probit cargado
                  predicted_probability = probit_2step_results.predict(last_second_stage_predictors)[0]

                  print(f"\nProbabilidad predicha de lluvia mañana para el último punto de datos disponible: {predicted_probability:.4f}")

                  # Realizar una predicción binaria basada en un umbral (por ejemplo, 0.5)
                  rain_prediction = "Sí" if predicted_probability >= 0.5 else "No"
                  print(f"Predicción de lluvia mañana (umbral >= 0.5): {rain_prediction}")

              else:
                  print("Error: El modelo de la primera etapa ('model_fs') no está disponible en el entorno actual.")
                  print("No se puede hacer la predicción para 'tp' para el último punto de datos.")
                  print("Para solucionarlo, necesitaría guardar también el modelo de la primera etapa de la celda L-mA11wjyr_l.")

"""### Otras regresiones usadas para ver que tan plausible es usar IV"""

# --- Regresión Lineal (Mínimos Cuadrados Ordinarios - OLS) ---
# Esta sección realiza una regresión lineal estándar de Mínimos Cuadrados Ordinarios (OLS)
# para modelar una de las variables climáticas continuas.

import statsmodels.api as sm
import pandas as pd

# Asumiendo que 'cartagena_df' ya está cargado y preprocesado.
# Asegurarse de que el DataFrame esté disponible.

if 'cartagena_df' not in locals() and 'cartagena_df' not in globals():
      print("Error: No se encontró el DataFrame 'cartagena_df'.")
      print("Por favor, asegúrese de que se ejecutaron los pasos de carga y preprocesamiento de datos.")
else:
    # Definir la variable dependiente (por ejemplo, temperatura a 2m)
    dependent_variable = 't2m'

    # Definir las variables independientes (predictoras)
    # Elegiremos algunas variables relevantes. Puede agregar o eliminar variables aquí.
    independent_variables = ['msl', 'si10', 'swvl1', 'sst'] # Predictoras de ejemplo

    # Agregar un término constante a las variables independientes (para el intercepto)
    X = sm.add_constant(cartagena_df[independent_variables])
    y = cartagena_df[dependent_variable]

    # Eliminar filas con valores faltantes en las variables seleccionadas
    # Esto asegura que el modelo OLS se pueda ajustar sin problemas
    data_for_ols = pd.concat([y, X], axis=1).dropna()
    y_clean = data_for_ols[dependent_variable]
    X_clean = data_for_ols.drop(columns=[dependent_variable])

    print(f"Ejecutando la regresión OLS para '{dependent_variable}'...")
    print(f"Número de observaciones utilizadas: {len(y_clean)}")

    # Ajustar el modelo OLS
    try:
        ols_model = sm.OLS(y_clean, X_clean).fit()

        # Imprimir el resumen de la regresión
        print("\n--- Resultados de la Regresión OLS ---")
        print(ols_model.summary())

    except Exception as e:
        print(f"\nError al ajustar el modelo OLS: {e}")

# --- Crear/Confirmar Columna de Tendencia Temporal para t2m ---

import pandas as pd
import numpy as np

# Asumiendo que 'cartagena_df' ya está cargado y preprocesado.
# Asegurarse de que el DataFrame esté disponible.

if 'cartagena_df' not in locals() and 'cartagena_df' not in globals():
      print("Error: No se encontró el DataFrame 'cartagena_df'.")
      print("Por favor, asegúrese de que se ejecutaron los pasos de carga y preprocesamiento de datos.")
else:
    # Verificar si la columna 'trend' ya existe.
    # Si no, crear una tendencia numérica simple basada en el índice.
    if 'trend' not in cartagena_df.columns:
        cartagena_df['trend'] = np.arange(len(cartagena_df))
        print("Columna 'trend' creada como un índice numérico.")
    else:
        print("La columna 'trend' ya existe.")

    # Mostrar las columnas 't2m' y 'trend' para visualizar su relación
    print("\nVista previa de las columnas 't2m' y 'trend':")
    display(cartagena_df[['t2m', 'trend']].head())
    display(cartagena_df[['t2m', 'trend']].tail())

# --- Regresión Lineal Simple ---
# Esta sección realiza una regresión lineal simple con una variable independiente.

import statsmodels.api as sm
import pandas as pd

# Asumiendo que 'cartagena_df' ya está cargado y preprocesado, incluyendo la columna 'trend'.
# Asegurarse de que el DataFrame esté disponible.

if 'cartagena_df' not in locals() and 'cartagena_df' not in globals():
      print("Error: No se encontró el DataFrame 'cartagena_df'.")
      print("Por favor, asegúrese de que se ejecutaron los pasos de carga y preprocesamiento de datos.")
elif 'trend' not in cartagena_df.columns:
      print("Error: No se encontró la columna 'trend' en 'cartagena_df'.")
      print("Por favor, asegúrese de que la columna 'trend' fue creada en un paso anterior.")
else:
    # Definir la variable dependiente (por ejemplo, temperatura a 2m)
    dependent_variable = 't2m'

    # Definir la variable independiente (por ejemplo, tendencia temporal)
    independent_variable = 'trend'

    # Agregar un término constante a la variable independiente (para el intercepto)
    X = sm.add_constant(cartagena_df[independent_variable])
    y = cartagena_df[dependent_variable]

    # Eliminar filas con valores faltantes en las variables seleccionadas
    data_for_simple_ols = pd.concat([y, X], axis=1).dropna()
    y_clean_simple = data_for_simple_ols[dependent_variable]
    X_clean_simple = data_for_simple_ols.drop(columns=[dependent_variable])


    print(f"Ejecutando la regresión OLS simple para '{dependent_variable}' en '{independent_variable}'...")
    print(f"Número de observaciones utilizadas: {len(y_clean_simple)}")

    # Ajustar el modelo OLS
    try:
        simple_ols_model = sm.OLS(y_clean_simple, X_clean_simple).fit()

        # Imprimir el resumen de la regresión
        print("\n--- Resultados de la Regresión OLS Simple ---")
        print(simple_ols_model.summary())

    except Exception as e:
        print(f"\nError al ajustar el modelo OLS simple: {e}")

# --- Predicción del Clima de Mañana ---

import pandas as pd
import numpy as np
import statsmodels.api as sm
import pickle

# --- 1. Predecir la Probabilidad de Lluvia Mañana usando el Modelo Probit IV ---

# Cargar los resultados del modelo Probit ajustado
try:
    with open('probit_2step_rain_tomorrow.pkl', 'rb') as f:
        probit_2step_results = pickle.load(f)
    print("Modelo Probit cargado exitosamente.")

except FileNotFoundError:
    print("Error: No se encontró el archivo del modelo 'probit_2step_rain_tomorrow.pkl'.")
    print("Por favor, asegúrese de que la celda anterior (L-mA11wjyr_l) se ejecutó correctamente para guardar el modelo.")
    probit_2step_results = None
except Exception as e:
    print(f"Ocurrió un error al cargar el modelo: {e}")
    probit_2step_results = None


if probit_2step_results is not None:
    # Necesitamos las variables predictoras para el *último* punto de datos disponible
    # en el 'cartagena_df' original para predecir la probabilidad del día siguiente.

    # Asegurarse de que 'cartagena_df' esté disponible y tenga las columnas necesarias
    if 'cartagena_df' not in locals() and 'cartagena_df' not in globals():
          print("Error: No se encontró el DataFrame 'cartagena_df'.")
          print("Por favor, asegúrese de que se ejecutaron los pasos de carga y preprocesamiento de datos.")
    else:
        # Obtener la última fila del DataFrame original
        # Usando el último punto de datos válido del dataframe usado para el ajuste
        # Esto asegura que tenemos todas las columnas requeridas sin NaNs
        # (Asume que df_model_probit_2step del paso anterior todavía está en memoria)
        if 'df_model_probit_2step' not in locals() and 'df_model_probit_2step' not in globals():
             print("Error: No se encontró el DataFrame 'df_model_probit_2step' usado para ajustar el modelo.")
             print("No se puede obtener el último punto de datos para la predicción.")
        else:
              last_data_point = df_model_probit_2step.iloc[[-1]].copy()

              # Reconstruir los predictores para la primera etapa para este último punto
              # (Asume que first_stage_predictors_clean_2step del paso anterior todavía está en memoria)
              if 'first_stage_predictors_clean_2step' not in locals() and 'first_stage_predictors_clean_2step' not in globals():
                   print("Error: No se encontró el DataFrame 'first_stage_predictors_clean_2step'.")
                   print("No se puede predecir la variable endógena para el último punto de datos.")
              else:
                    last_first_stage_predictors = first_stage_predictors_clean_2step.iloc[[-1]].copy()

                    # Necesitamos los coeficientes del *modelo OLS de la primera etapa* para predecir 'tp'.
                    # El modelo de la primera etapa ('model_fs') fue ajustado dentro de la celda anterior (L-mA11wjyr_l).
                    # Si 'model_fs' no está en el entorno, necesitaríamos reajustar o cargar.
                    # Para demostración, asumimos que está disponible.

                    if 'model_fs' in locals() or 'model_fs' in globals():
                        print("\nRealizando predicción de probabilidad de lluvia mañana...")
                        predicted_tp_last = model_fs.predict(last_first_stage_predictors)

                        # Reconstruir los predictores de la segunda etapa para el último punto de datos
                        last_exog_probit_clean_2step = exog_probit_clean_2step.iloc[[-1]].copy()
                        last_second_stage_predictors = pd.concat([last_exog_probit_clean_2step, predicted_tp_last.rename('tp_predicted')], axis=1)

                        # Asegurarse de que los nombres de las columnas coincidan exactamente con los datos de entrenamiento del modelo
                        last_second_stage_predictors.columns = probit_2step_results.params.index

                        # Predecir la probabilidad usando el modelo Probit cargado
                        predicted_probability = probit_2step_results.predict(last_second_stage_predictors)[0]

                        print(f"Probabilidad predicha de lluvia mañana: {predicted_probability:.4f}")

                        # Realizar una predicción binaria basada en un umbral (por ejemplo, 0.5)
                        rain_prediction = "Sí" if predicted_probability >= 0.5 else "No"
                        print(f"Predicción de lluvia mañana (umbral >= 0.5): {rain_prediction}")

                    else:
                        print("Error: El modelo de la primera etapa ('model_fs') no está disponible en el entorno actual.")
                        print("No se puede hacer la predicción para 'tp' para el último punto de datos, y por lo tanto, la predicción Probit.")

# --- 2. Predecir Temperatura Horaria para Mañana ---
# Como no tenemos un modelo de regresión IV para la temperatura horaria,
# usaremos un enfoque simple: el patrón de temperatura del último día completo disponible.

print("\nGenerando predicción de temperatura horaria para mañana (basada en el último día completo)...")

# Encontrar la fecha del último día completo en el DataFrame
# La última marca de tiempo es 2025-08-02 00:00:00. El último día completo es 2025-08-01.
if 'cartagena_df' in locals() or 'cartagena_df' in globals():
    if not cartagena_df.empty:
        ultima_fecha_completa = cartagena_df.index.max().normalize() - pd.Timedelta(days=1) # Fecha del día anterior al último punto disponible
        fecha_manana = cartagena_df.index.max().normalize() # La fecha que sigue al último punto disponible

        # Filtrar los datos del último día completo
        last_day_data = cartagena_df[cartagena_df.index.date == ultima_fecha_completa.date()].copy()

        if not last_day_data.empty:
            print(f"Usando patrón de temperatura del día {ultima_fecha_completa.date()}")

            # Crear un DataFrame para la predicción de mañana
            # Usar las horas del último día completo, pero con la fecha de mañana
            predicted_hourly_temp_tomorrow = last_day_data[['t2m']].copy()
            predicted_hourly_temp_tomorrow.index = predicted_hourly_temp_tomorrow.index.map(lambda t: t.replace(year=fecha_manana.year, month=fecha_manana.month, day=fecha_manana.day))

            print("\nPredicción de Temperatura Horaria para Mañana (basada en el último día completo):")
            display(predicted_hourly_temp_tomorrow)

            # Opcional: Visualizar la predicción horaria
            plt.figure(figsize=(12, 6))
            plt.plot(predicted_hourly_temp_tomorrow.index, predicted_hourly_temp_tomorrow['t2m'], marker='o', linestyle='-')
            plt.title(f'Predicción de Temperatura Horaria para {fecha_manana.date()}', fontsize=16)
            plt.xlabel("Hora (UTC)", fontsize=12)
            plt.ylabel("Temperatura (°C)", fontsize=12)
            plt.xticks(predicted_hourly_temp_tomorrow.index) # Mostrar todas las horas si son pocas, o usar un DateFormatter
            plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
            plt.grid(True, linestyle='--')
            plt.tight_layout()
            plt.show()

             # --- 3. Identificar las horas con mayor probabilidad de lluvia mañana ---
            print("\nIdentificando horas con mayor probabilidad de lluvia mañana...")

            # Usamos la predicción binaria de lluvia_prediction (Sí/No) del modelo Probit IV
            # Si el modelo Probit IV predice "Sí" para lluvia mañana, entonces buscaremos
            # patrones de precipitación en el último día completo o simplemente indicaremos
            # que se espera lluvia a lo largo del día.

            if rain_prediction == "Sí":
                print(f"Según el modelo Probit IV, es probable que llueva mañana ({fecha_manana.date()}).")
                # Como no tenemos un modelo horario de precipitación, una simplificación es
                # asumir que la lluvia podría ocurrir en cualquier momento, o si queremos ser
                # más específicos, podríamos buscar las horas donde hubo más precipitación
                # en el último día completo (last_day_data['tp']).

                # Aquí, simplemente listaremos las horas donde hubo precipitación > umbral
                # en el *último día completo*, como una indicación de posibles horas.
                # NOTA: Esto NO es una predicción directa del modelo IV horario.
                # Es una inferencia basada en el patrón del último día observado + predicción binaria.

                hours_with_rain_last_day = last_day_data[last_day_data['tp'] > rain_threshold].index.strftime('%H:%M').tolist()

                if hours_with_rain_last_day:
                     print(f"Basado en el patrón de precipitación del último día completo ({ultima_fecha_completa.date()}), las horas (UTC) con precipitación fueron: {', '.join(hours_with_rain_last_day)}")
                else:
                     print(f"Aunque se predice lluvia, no se registraron precipitaciones por encima del umbral ({rain_threshold}m) en el último día completo ({ultima_fecha_completa.date()}).")
                     print("Considere esto una indicación general de lluvia en el día.")

            else:
                print(f"Según el modelo Probit IV, no se espera lluvia mañana ({fecha_manana.date()}).")

        else:
            print(f"No se encontraron datos para el último día completo ({ultima_fecha_completa.date()}). No se puede predecir la temperatura horaria ni inferir horas de lluvia.")
    else:
        print("El DataFrame 'cartagena_df' está vacío. No se puede predecir el clima de mañana.")
else:
    print("El DataFrame 'cartagena_df' no está disponible.")


print("\nProceso de predicción de clima para mañana completado.")

# Filter cartagena_df for the specific date 2025-08-02
date_to_display = '2025-08-01'
temperature_data_2025_08_01 = cartagena_df[cartagena_df.index.date == pd.to_datetime(date_to_display).date()]

print(f"Temperatura para el día {date_to_display}:")
display(temperature_data_2025_08_01[['t2m']])

# Check the shape of the filtered DataFrame to see how many data points are available for the day
print(f"\nShape of data for {date_to_display}: {temperature_data_2025_08_01.shape}")

import matplotlib.pyplot as plt

if not temperature_data_2025_08_01.empty:
    plt.figure(figsize=(12, 6))
    plt.plot(temperature_data_2025_08_01.index, temperature_data_2025_08_01['t2m'], marker='o', linestyle='-')
    plt.title(f'Actual Temperatura Horaria para {date_to_display}', fontsize=16)
    plt.xlabel("Hora (UTC)", fontsize=12)
    plt.ylabel("Temperatura (°C)", fontsize=12)
    plt.xticks(temperature_data_2025_08_01.index)
    plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
    plt.grid(True, linestyle='--')
    plt.tight_layout()
    plt.show()
else:
    print(f"No se encontraron datos de temperatura para el día {date_to_display}.")